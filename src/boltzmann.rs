use num::{Float, one, zero};

use rand::{Rand, random, thread_rng};
use rand::distributions::{IndependentSample, Range};

use SymmetricMatrix;

/// A stochastic self-organizing network.
///
/// Given a set of constraints defined as the weigths symmetric matrix
/// and the biases vector, find a solution as a vector of zeroes and one
/// (activated and deactivated neurons).
///
/// A positive coefficient between to neurons will tend to have an activated one
/// bring the other to be activated as well, a negative coefficient will tend to
/// have an activated one to deactivate the other.
///
/// Likewise, a positive bias will tend to have the unit activated, a negative bias
/// to have it deactivated.
///
/// The process is probabilistic and will tend to minimising the unsatisfied constraint.
pub struct BoltzmannMachine<F: Float> {
    values: Vec<F>,
    biases: Vec<F>,
    coeffs: SymmetricMatrix<F>
}

impl<F: Float> BoltzmannMachine<F> {
    /// Creates a new machine with given set of weights and biases set to zero.
    pub fn new(weigths: SymmetricMatrix<F>) -> BoltzmannMachine<F> {
        let n = weigths.size();
        BoltzmannMachine {
            values: vec![one(); n],
            biases: vec![zero(); n],
            coeffs: weigths
        }
    }

    /// Creates a new machine with given weigths and biases.
    pub fn with_biases(weigths: SymmetricMatrix<F>, biases: Vec<F>) -> BoltzmannMachine<F> {
        let n = weigths.size();
        assert!(biases.len() == n, "The biases count must be equal to the nodes count.");
        BoltzmannMachine {
            values: vec![one(); n],
            biases: biases,
            coeffs: weigths
        }
    }

    /// Get access to the values of the neurons.
    ///
    /// All values generated by the algorithm are either `0.0` or `1.0`.
    pub fn values(&self) -> &[F] {
        &self.values
    }

    /// Get mutable access to the values of the neurons.
    ///
    /// You can use this to set and initial state of the machine, or to enforce
    /// a set of neurons (the inputs) which values should never change.
    pub fn values_mut(&mut self) -> &mut [F] {
        &mut self.values
    }
}

impl<F: Float + Rand> BoltzmannMachine<F> {
    /// Sequentially update all neurons of the machine, always in the same order, excluding the indices
    /// provided in the `exclude` parameter.
    ///
    /// This method will converge faster than the random one, but may give poorer results and get more
    /// easily  stuck in a local minimum.
    ///
    /// See `tick_one_random(..)` for explanations on the `temperature` parameter.
    pub fn tick_all_sequential(&mut self, temperature: F,  exclude: &[usize]) {
        let n = self.values.len();
        for i in 0..n {
            if exclude.contains(&i) { continue; }
            let mut val = self.biases[i];
            for j in 0..n {
                if i!=j {
                    val = val + self.values[j] * self.coeffs[(i,j)];
                }
            }
            val = -val / temperature;
            if random::<F>() < (one::<F>() + val.exp()).recip() {
                val = one::<F>();
            } else {
                val = zero::<F>();
            };
            self.values[i] = val;
        }
    }

    /// Updates a random neuron of the network, excluding the indices provided
    /// in the `exclude` parameter.
    ///
    /// The `temperature` parameter controls the stability of the evolution:
    ///
    /// - a high temperature will make the constraints very ligth, and allow the system
    ///   to explore more states
    /// - a low temperature will on the contrary make the constrains very strong, and enforce
    ///   them on the changes of states of the neurons.
    ///
    /// A typical use would be to start with a high temperature and slowly decrease it, as
    ///  the network "cools down" and stabilizes in a state satisfying the constraints.
    ///
    /// Putting a negative temperature would be similar in effect as multiplying all
    /// weigths and biases by `-1.0`.
    pub fn tick_one_random(&mut self, temperature: F, exclude: &[usize]) {
        let n = self.biases.len();
        let limits = Range::<usize>::new(0, n);
        let mut rng = thread_rng();
        let mut idx = limits.ind_sample(&mut rng);
        while exclude.contains(&idx) {
            idx = limits.ind_sample(&mut rng);
        }
        let mut val = self.biases[idx];
        for j in 0..n {
            if idx!=j {
                val = val + self.values[j] * self.coeffs[(idx,j)];
            }
        }
        val = -val / temperature;
        if random::<F>() < (one::<F>() + val.exp()).recip() {
            val = one::<F>();
        } else {
            val = zero::<F>();
        };
        self.values[idx] = val;
    }
}
